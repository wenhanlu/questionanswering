{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, I tried implementing a Hugging Face extractive question answerer using the bAbI tasks generated by Meta's AI Research division to advance NLP. Question answering is a branch of NLP that seeks to train models to process questions, understand its meaning, and generate (or in this case, identify the portion of the text that contains) the correct answer. This is my first venture into NLP, done as a learning experience for myself, and the contents of this notebook are inspired by the material I learned from DeepLearning.AI's class on Coursera. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport os\nfrom copy import deepcopy\nfrom datasets import load_from_disk, Dataset\n#from datasets.dataset_dict import DatasetDic\nfrom transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering, Trainer, TrainingArguments\nfrom sklearn.metrics import f1_score\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:11.706401Z","iopub.execute_input":"2023-10-02T09:01:11.707090Z","iopub.status.idle":"2023-10-02T09:01:26.815951Z","shell.execute_reply.started":"2023-10-02T09:01:11.707062Z","shell.execute_reply":"2023-10-02T09:01:26.814949Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The format of this data is as follows:\n- answer: the one-word answer to the question\n- id: each sentence within the story has its own id starting from 1\n- supporting_ids: the sentence id that helps answer the question\n- text: the English text of each sentence\n- type: 0 if the sentence gives context and 1 if the sentence is the question","metadata":{}},{"cell_type":"code","source":"babi_dataset = load_from_disk('/kaggle/input/babl-question-answering/data')\nprint(type(babi_dataset))\nfor i in range(3):\n    idx = random.randint(0, len(babi_dataset['train']))\n    print('index:', idx)\n    print(babi_dataset['train'][idx])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:26.818016Z","iopub.execute_input":"2023-10-02T09:01:26.818424Z","iopub.status.idle":"2023-10-02T09:01:26.880382Z","shell.execute_reply.started":"2023-10-02T09:01:26.818387Z","shell.execute_reply":"2023-10-02T09:01:26.879414Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"<class 'datasets.dataset_dict.DatasetDict'>\nindex: 420\n{'story': {'answer': ['', '', 'kitchen'], 'id': ['1', '2', '3'], 'supporting_ids': [[], [], ['1']], 'text': ['The kitchen is west of the hallway.', 'The garden is east of the hallway.', 'What is the hallway east of?'], 'type': [0, 0, 1]}}\nindex: 77\n{'story': {'answer': ['', '', 'bathroom'], 'id': ['1', '2', '3'], 'supporting_ids': [[], [], ['1']], 'text': ['The bathroom is south of the garden.', 'The kitchen is north of the garden.', 'What is the garden north of?'], 'type': [0, 0, 1]}}\nindex: 57\n{'story': {'answer': ['', '', 'garden'], 'id': ['1', '2', '3'], 'supporting_ids': [[], [], ['1']], 'text': ['The garden is west of the bedroom.', 'The office is west of the garden.', 'What is west of the bedroom?'], 'type': [0, 0, 1]}}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The last element of each entry in the DatasetDict is called 'type', which indicates whether the corresponding sentence is supposed to be a contextual sentence (0) or a question (1). We can check all of the unique formats of the sentences by adding them to a set. Luckily in this dataset, they're all the same.","metadata":{}},{"cell_type":"code","source":"type_set = set()\nfor story in babi_dataset['train']:\n    if str(story['story']['type'] )not in type_set:\n        type_set.add(str(story['story']['type'] ))\ntype_set","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:26.881600Z","iopub.execute_input":"2023-10-02T09:01:26.882182Z","iopub.status.idle":"2023-10-02T09:01:26.976438Z","shell.execute_reply.started":"2023-10-02T09:01:26.882153Z","shell.execute_reply":"2023-10-02T09:01:26.975458Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'[0, 0, 1]'}"},"metadata":{}}]},{"cell_type":"markdown","source":"Flattening from dictionary to table structure","metadata":{}},{"cell_type":"code","source":"flattened_babi = babi_dataset.flatten()\nprint(flattened_babi)\nnext(iter(flattened_babi['train']))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:26.978903Z","iopub.execute_input":"2023-10-02T09:01:26.979513Z","iopub.status.idle":"2023-10-02T09:01:26.992451Z","shell.execute_reply.started":"2023-10-02T09:01:26.979465Z","shell.execute_reply":"2023-10-02T09:01:26.991281Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['story.answer', 'story.id', 'story.supporting_ids', 'story.text', 'story.type'],\n        num_rows: 1000\n    })\n    test: Dataset({\n        features: ['story.answer', 'story.id', 'story.supporting_ids', 'story.text', 'story.type'],\n        num_rows: 1000\n    })\n})\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'story.answer': ['', '', 'office'],\n 'story.id': ['1', '2', '3'],\n 'story.supporting_ids': [[], [], ['1']],\n 'story.text': ['The office is north of the kitchen.',\n  'The garden is south of the kitchen.',\n  'What is north of the kitchen?'],\n 'story.type': [0, 0, 1]}"},"metadata":{}}]},{"cell_type":"markdown","source":"For some reason I couldn't use map() or add new keys to the flattened_babi dataset, so I made a new dictionary with three new keys instead of using a DatasetDic (which is what Hugging Face uses). We can now extract individual questions, answers, as well as the contextual sentences with their keys. ","metadata":{}},{"cell_type":"code","source":"train_data = []\nfor original_dict in flattened_babi['train']:\n    copy_dict = deepcopy(original_dict)\n    copy_dict.update({\n        'questions': original_dict['story.text'][2],\n        'sentences': ' '.join([original_dict['story.text'][0], original_dict['story.text'][1]]),\n        'answer': original_dict['story.answer'][2]\n    })\n    train_data.append(copy_dict)\n\ntest_data = []\nfor original_dict in flattened_babi['test']:\n    copy_dict = deepcopy(original_dict)\n    copy_dict.update({\n        'questions': original_dict['story.text'][2],\n        'sentences': ' '.join([original_dict['story.text'][0], original_dict['story.text'][1]]),\n        'answer': original_dict['story.answer'][2]\n    })\n    test_data.append(copy_dict)\n    \nprocessed = {'train': train_data, 'test': test_data}","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:26.993781Z","iopub.execute_input":"2023-10-02T09:01:26.994293Z","iopub.status.idle":"2023-10-02T09:01:27.248015Z","shell.execute_reply.started":"2023-10-02T09:01:26.994263Z","shell.execute_reply":"2023-10-02T09:01:27.247124Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at some entries in our dataset","metadata":{}},{"cell_type":"code","source":"for i in range(3):\n    idx = random.randint(0, len(babi_dataset['train']))\n    print('index:', idx)\n    print('train:', processed['train'][idx])\n    print('test:', processed['test'][idx])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:27.249298Z","iopub.execute_input":"2023-10-02T09:01:27.249838Z","iopub.status.idle":"2023-10-02T09:01:27.256558Z","shell.execute_reply.started":"2023-10-02T09:01:27.249807Z","shell.execute_reply":"2023-10-02T09:01:27.255459Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"index: 789\ntrain: {'story.answer': ['', '', 'office'], 'story.id': ['1', '2', '3'], 'story.supporting_ids': [[], [], ['1']], 'story.text': ['The office is east of the garden.', 'The bathroom is west of the garden.', 'What is east of the garden?'], 'story.type': [0, 0, 1], 'questions': 'What is east of the garden?', 'sentences': 'The office is east of the garden. The bathroom is west of the garden.', 'answer': 'office'}\ntest: {'story.answer': ['', '', 'office'], 'story.id': ['1', '2', '3'], 'story.supporting_ids': [[], [], ['2']], 'story.text': ['The kitchen is north of the bedroom.', 'The office is south of the bedroom.', 'What is the bedroom north of?'], 'story.type': [0, 0, 1], 'questions': 'What is the bedroom north of?', 'sentences': 'The kitchen is north of the bedroom. The office is south of the bedroom.', 'answer': 'office'}\nindex: 938\ntrain: {'story.answer': ['', '', 'hallway'], 'story.id': ['1', '2', '3'], 'story.supporting_ids': [[], [], ['2']], 'story.text': ['The bedroom is west of the bathroom.', 'The hallway is west of the bedroom.', 'What is west of the bedroom?'], 'story.type': [0, 0, 1], 'questions': 'What is west of the bedroom?', 'sentences': 'The bedroom is west of the bathroom. The hallway is west of the bedroom.', 'answer': 'hallway'}\ntest: {'story.answer': ['', '', 'kitchen'], 'story.id': ['1', '2', '3'], 'story.supporting_ids': [[], [], ['1']], 'story.text': ['The kitchen is south of the bedroom.', 'The hallway is north of the bedroom.', 'What is south of the bedroom?'], 'story.type': [0, 0, 1], 'questions': 'What is south of the bedroom?', 'sentences': 'The kitchen is south of the bedroom. The hallway is north of the bedroom.', 'answer': 'kitchen'}\nindex: 13\ntrain: {'story.answer': ['', '', 'hallway'], 'story.id': ['1', '2', '3'], 'story.supporting_ids': [[], [], ['2']], 'story.text': ['The garden is west of the hallway.', 'The hallway is west of the office.', 'What is the office east of?'], 'story.type': [0, 0, 1], 'questions': 'What is the office east of?', 'sentences': 'The garden is west of the hallway. The hallway is west of the office.', 'answer': 'hallway'}\ntest: {'story.answer': ['', '', 'garden'], 'story.id': ['1', '2', '3'], 'story.supporting_ids': [[], [], ['2']], 'story.text': ['The bathroom is east of the kitchen.', 'The garden is west of the kitchen.', 'What is west of the kitchen?'], 'story.type': [0, 0, 1], 'questions': 'What is west of the kitchen?', 'sentences': 'The bathroom is east of the kitchen. The garden is west of the kitchen.', 'answer': 'garden'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now adding the starting and ending indices for each story. This is important because we're doing extractive question answering, which relies on identifying the position of the answer within the contextual sentences. ","metadata":{}},{"cell_type":"code","source":"for story in processed['train']:\n    story.update({\n        'str_idx': story['sentences'].find(story['answer']),\n        'end_idx': story['sentences'].find(story['answer']) + len(story['answer'])\n    })\nfor story in processed['test']:\n    story.update({\n        'str_idx': story['sentences'].find(story['answer']),\n        'end_idx': story['sentences'].find(story['answer']) + len(story['answer'])\n    })","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:27.258110Z","iopub.execute_input":"2023-10-02T09:01:27.258820Z","iopub.status.idle":"2023-10-02T09:01:27.268837Z","shell.execute_reply.started":"2023-10-02T09:01:27.258786Z","shell.execute_reply":"2023-10-02T09:01:27.267722Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Now that we're done processing the dataset, we have to tokenize and align the input for the model. This is necessary because language models process text in fixed-size chunks. Since words have different lengths, tokenization converts the text into a sequence of tokens, allowing it to be processed by the model. More importantly, it also maps unique words from readable English to their own vectors for the model to understand and use. The DistilBert fast tokenizer uses sequences of length 512 and pads with zeros when necessary.\n\n","metadata":{}},{"cell_type":"code","source":"tokenizer = DistilBertTokenizerFast.from_pretrained('/kaggle/input/babl-question-answering/tokenizer')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:27.270211Z","iopub.execute_input":"2023-10-02T09:01:27.271324Z","iopub.status.idle":"2023-10-02T09:01:27.365209Z","shell.execute_reply.started":"2023-10-02T09:01:27.271292Z","shell.execute_reply":"2023-10-02T09:01:27.364250Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"One thing I didn't mention about tokenizers is that they often split words into several subwords. One must be very careful when tokenizing to avoid misaligning ","metadata":{}},{"cell_type":"code","source":"train_data = []\nfor original_dict in processed['train']:\n\n    encoding = tokenizer(original_dict['sentences'], original_dict['questions'], truncation=True, padding=True, max_length=tokenizer.model_max_length)\n    start_positions = encoding.char_to_token(original_dict['str_idx'])\n    end_positions = encoding.char_to_token(original_dict['end_idx']-1)\n    if start_positions is None:\n        start_positions = tokenizer.model_max_length\n    if end_positions is None:\n        end_positions = tokenizer.model_max_length\n    original_dict.update({\n        'input_ids': encoding['input_ids'],\n        'attention_mask': encoding['attention_mask'],\n        'start_positions': start_positions,\n        'end_positions': end_positions \n    })\n    train_data.append(original_dict)\n\ntest_data = []\nfor original_dict in processed['test']:\n\n    encoding = tokenizer(original_dict['sentences'], original_dict['questions'], truncation=True, padding=True, max_length=tokenizer.model_max_length)\n    start_positions = encoding.char_to_token(original_dict['str_idx'])\n    end_positions = encoding.char_to_token(original_dict['end_idx']-1)\n    if start_positions is None:\n        start_positions = tokenizer.model_max_length\n    if end_positions is None:\n        end_positions = tokenizer.model_max_length\n    original_dict.update({\n        'input_ids': encoding['input_ids'],\n        'attention_mask': encoding['attention_mask'],\n        'start_positions': start_positions,\n        'end_positions': end_positions \n    })\n    test_data.append(original_dict)\n    \nqa_dataset = {'train': train_data, 'test': test_data}\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:27.366358Z","iopub.execute_input":"2023-10-02T09:01:27.366888Z","iopub.status.idle":"2023-10-02T09:01:27.652152Z","shell.execute_reply.started":"2023-10-02T09:01:27.366850Z","shell.execute_reply":"2023-10-02T09:01:27.651238Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Getting rid of the preprocessed data in favor of the new data we created earlier","metadata":{}},{"cell_type":"code","source":"keys_to_delete = ['story.answer', 'story.id', 'story.supporting_ids', 'story.text', 'story.type']\nfor original_dict in qa_dataset['train']:\n    for key in keys_to_delete:\n        del original_dict[key]\nfor original_dict in qa_dataset['test']:\n    for key in keys_to_delete:\n        del original_dict[key]","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:27.655672Z","iopub.execute_input":"2023-10-02T09:01:27.655946Z","iopub.status.idle":"2023-10-02T09:01:27.661893Z","shell.execute_reply.started":"2023-10-02T09:01:27.655924Z","shell.execute_reply":"2023-10-02T09:01:27.660946Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Since map() didn't work, I had to use a dictionary instead of DatasetDict to store the training/test sets. This step is to convert the train/test datasets back to the proper datatype for the Hugging Face model to use. There's already a DistilBert model specifically for question answering so I'm using that for fine tuning.","metadata":{}},{"cell_type":"code","source":"train_ds = Dataset.from_pandas(pd.DataFrame(data=qa_dataset['train']))\ntest_ds = Dataset.from_pandas(pd.DataFrame(data=qa_dataset['test']))\n\nmodel = DistilBertForQuestionAnswering.from_pretrained(\"/kaggle/input/babl-question-answering/model/pytorch\")","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:27.663324Z","iopub.execute_input":"2023-10-02T09:01:27.664158Z","iopub.status.idle":"2023-10-02T09:01:30.375639Z","shell.execute_reply.started":"2023-10-02T09:01:27.664128Z","shell.execute_reply":"2023-10-02T09:01:30.374130Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Choosing the variables of interest and changing the format for a PyTorch implementation","metadata":{}},{"cell_type":"code","source":"columns_to_return = ['input_ids','attention_mask', 'start_positions', 'end_positions']\n\ntrain_ds.set_format(type='pt', columns=columns_to_return)\ntest_ds.set_format(type='pt', columns=columns_to_return)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:30.377535Z","iopub.execute_input":"2023-10-02T09:01:30.378277Z","iopub.status.idle":"2023-10-02T09:01:30.388775Z","shell.execute_reply.started":"2023-10-02T09:01:30.378221Z","shell.execute_reply":"2023-10-02T09:01:30.387487Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Using a custom function to compute the f1 score","metadata":{}},{"cell_type":"code","source":"def compute_metrics(pred):\n    start_labels = pred.label_ids[0]\n    start_preds = pred.predictions[0].argmax(-1)\n    end_labels = pred.label_ids[1]\n    end_preds = pred.predictions[1].argmax(-1)\n    \n    f1_start = f1_score(start_labels, start_preds, average='macro')\n    f1_end = f1_score(end_labels, end_preds, average='macro')\n    \n    return {\n        'f1_start': f1_start,\n        'f1_end': f1_end,\n    }","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:30.392903Z","iopub.execute_input":"2023-10-02T09:01:30.393676Z","iopub.status.idle":"2023-10-02T09:01:30.402889Z","shell.execute_reply.started":"2023-10-02T09:01:30.393638Z","shell.execute_reply":"2023-10-02T09:01:30.401851Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Training time","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='results',         \n    overwrite_output_dir=True,\n    num_train_epochs=5,             \n    per_device_train_batch_size=8,  \n    per_device_eval_batch_size=8,   \n    warmup_steps=20,                \n    weight_decay=0.01,               \n    logging_dir=None,           \n    logging_steps=50\n)\n\ntrainer = Trainer(\n    model=model,                \n    args=training_args,                \n    train_dataset=train_ds,        \n    eval_dataset=test_ds,\n    compute_metrics=compute_metrics            \n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:01:30.404329Z","iopub.execute_input":"2023-10-02T09:01:30.405022Z","iopub.status.idle":"2023-10-02T09:02:56.943994Z","shell.execute_reply.started":"2023-10-02T09:01:30.404988Z","shell.execute_reply":"2023-10-02T09:02:56.943037Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.11 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231002_090142-bpshzcw5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/wenhanlu/huggingface/runs/bpshzcw5' target=\"_blank\">feasible-lake-4</a></strong> to <a href='https://wandb.ai/wenhanlu/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/wenhanlu/huggingface' target=\"_blank\">https://wandb.ai/wenhanlu/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/wenhanlu/huggingface/runs/bpshzcw5' target=\"_blank\">https://wandb.ai/wenhanlu/huggingface/runs/bpshzcw5</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [315/315 00:35, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.480900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.559700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.429900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.417500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.277600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.173700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=315, training_loss=0.5409559946211557, metrics={'train_runtime': 79.883, 'train_samples_per_second': 62.592, 'train_steps_per_second': 3.943, 'total_flos': 33173638680000.0, 'train_loss': 0.5409559946211557, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"0.93 F1 score is pretty good!","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:02:56.945202Z","iopub.execute_input":"2023-10-02T09:02:56.950378Z","iopub.status.idle":"2023-10-02T09:02:59.471334Z","shell.execute_reply.started":"2023-10-02T09:02:56.950332Z","shell.execute_reply":"2023-10-02T09:02:59.469985Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:02]\n    </div>\n    "},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.14967407286167145,\n 'eval_f1_start': 0.9352821297160085,\n 'eval_f1_end': 0.936880101821786,\n 'eval_runtime': 2.5049,\n 'eval_samples_per_second': 399.214,\n 'eval_steps_per_second': 25.15,\n 'epoch': 5.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"Printing out some examples from the test set. It messes up a few but I'm pretty impressed.","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\nfor i in range(10):\n    idx = random.randint(0, len(test_ds))\n    text = test_ds['sentences'][idx]\n    question = test_ds['questions'][idx]\n    \n    input_dict = tokenizer(text, question, return_tensors='pt')\n\n    input_ids = input_dict['input_ids'].to(device)\n    attention_mask = input_dict['attention_mask'].to(device)\n\n    outputs = model(input_ids, attention_mask=attention_mask)\n\n    start_logits = outputs[0]\n    end_logits = outputs[1]\n\n    all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n    answer = ' '.join(all_tokens[torch.argmax(start_logits, 1)[0] : torch.argmax(end_logits, 1)[0]+1])\n\n    print(text)\n    print(question, answer.capitalize())\n    print('='*50)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:03:35.631568Z","iopub.execute_input":"2023-10-02T09:03:35.631909Z","iopub.status.idle":"2023-10-02T09:03:35.766015Z","shell.execute_reply.started":"2023-10-02T09:03:35.631881Z","shell.execute_reply":"2023-10-02T09:03:35.765099Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"The kitchen is south of the garden. The bedroom is south of the kitchen.\nWhat is the kitchen north of? Bedroom\n==================================================\nThe kitchen is west of the bathroom. The office is east of the bathroom.\nWhat is east of the bathroom? Office\n==================================================\nThe bathroom is west of the kitchen. The office is east of the kitchen.\nWhat is the kitchen east of? Bathroom\n==================================================\nThe kitchen is east of the office. The hallway is west of the office.\nWhat is west of the office? Hallway\n==================================================\nThe office is south of the bathroom. The bedroom is north of the bathroom.\nWhat is south of the bathroom? Office\n==================================================\nThe kitchen is north of the garden. The hallway is south of the garden.\nWhat is the garden north of? Hallway\n==================================================\nThe garden is north of the kitchen. The bathroom is south of the kitchen.\nWhat is the kitchen north of? Garden\n==================================================\nThe bathroom is west of the bedroom. The kitchen is west of the bathroom.\nWhat is west of the bedroom? Bathroom\n==================================================\nThe hallway is west of the bedroom. The kitchen is east of the bedroom.\nWhat is west of the bedroom? Hallway\n==================================================\nThe kitchen is east of the bathroom. The garden is west of the bathroom.\nWhat is west of the bathroom? Garden\n==================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained('/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T09:03:39.414224Z","iopub.execute_input":"2023-10-02T09:03:39.414584Z","iopub.status.idle":"2023-10-02T09:03:39.925404Z","shell.execute_reply.started":"2023-10-02T09:03:39.414558Z","shell.execute_reply":"2023-10-02T09:03:39.924407Z"},"trusted":true},"execution_count":19,"outputs":[]}]}